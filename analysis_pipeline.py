# -*- coding: utf-8 -*-
"""analysis_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fWoO9kMHxglzxoS0RtO0EBlsEZX4SUGy
"""

# analysis_pipeline.py
"""
KDD Supplementary Materials - Reproducible Analysis Pipeline (Tables S1–S3)

This script is aligned with the paper abstract and the (revised) READMEs:
- 34 Korean risk keywords (validated lexicon; used for consistency checks)
- 94.1% overall detection rate (computed from session-level metadata)
- Kaplan–Meier survival analysis
- Bootstrap confidence intervals (1,000 iterations)
- Minimum Safety Window (MSW): turn where 50% of at-risk sessions disclose risk

Inputs (in --data_dir):
- session_metadata.csv   (required)
- keyword_lexicon.csv    (optional but recommended; must contain 34 unique keywords)

Outputs (to --output_dir):
- table_s1_dataset_characteristics.xlsx
- table_s2_msw_estimates.xlsx
- table_s3_turn_budget_sensitivity.xlsx

No figures are produced.
"""

from __future__ import annotations

from pathlib import Path
import argparse
import numpy as np
import pandas as pd
from lifelines import KaplanMeierFitter
from tqdm import tqdm

# -----------------------------
# Config (EDIT IF NEEDED)
# -----------------------------
CRISIS_MAPPING = {
    "정상군": "Normal",
    "관찰필요": "Observation",
    "상담필요": "Counseling",
    "학대의심": "Abuse-Suspected",
    "응급": "Emergency",
}

CRISIS_LEVELS = ["Normal", "Observation", "Counseling", "Abuse-Suspected", "Emergency"]

# Turn thresholds used in Table S3
TURN_THRESHOLDS = [10, 20, 30, 40, 50, 60, 80, 100, 120]

# Inputs
METADATA_FILENAME = "session_metadata.csv"
LEXICON_FILENAME = "keyword_lexicon.csv"  # optional check
KEYWORD_COL = "keyword_ko"                # adjust to your CSV column name if needed

# Bootstrap
BOOTSTRAP_ITER = 1000
BOOTSTRAP_SEED = 42

# -----------------------------
# Utilities
# -----------------------------
def _coerce_bool01(series: pd.Series) -> pd.Series:
    """
    Robust coercion to boolean from common encodings:
    - 0/1
    - "0"/"1"
    - True/False
    - "true"/"false"
    """
    if series.dtype == bool:
        return series

    s = series.copy()

    # numeric-like
    try:
        s_num = pd.to_numeric(s, errors="coerce")
        if s_num.notna().any():
            return s_num.fillna(0).astype(int).astype(bool)
    except Exception:
        pass

    # string-like
    s_str = s.astype(str).str.strip().str.lower()
    truthy = {"1", "true", "t", "yes", "y"}
    falsy = {"0", "false", "f", "no", "n", "nan", "none", ""}

    out = []
    for v in s_str.tolist():
        if v in truthy:
            out.append(True)
        elif v in falsy:
            out.append(False)
        else:
            # conservative default: treat unknown tokens as False
            out.append(False)
    return pd.Series(out, index=series.index, dtype=bool)


def load_keywords_csv(csv_path: Path, keyword_col: str) -> list[str]:
    """
    Optional consistency check: enforce 34 unique keywords.
    This does NOT affect the KM/MSW computation (which is based on session_metadata.csv).
    """
    lex = pd.read_csv(csv_path)
    if keyword_col not in lex.columns:
        raise ValueError(
            f"Keyword column '{keyword_col}' not found in {csv_path}. "
            f"Columns: {list(lex.columns)}"
        )

    keywords = (
        lex[keyword_col]
        .dropna()
        .astype(str)
        .map(str.strip)
        .tolist()
    )

    # remove empties, de-duplicate while preserving order
    seen = set()
    out: list[str] = []
    for k in keywords:
        if not k:
            continue
        if k not in seen:
            out.append(k)
            seen.add(k)

    if len(out) != 34:
        raise ValueError(
            f"Expected 34 keywords, got {len(out)}. "
            f"Check {csv_path.name} or KEYWORD_COL."
        )
    return out


def load_session_metadata(csv_path: Path) -> pd.DataFrame:
    """
    Expected columns (README/abstract-aligned):
      - crisis_level (Korean labels or English; will be mapped to English levels)
      - total_turns
      - disclosed (0/1 or bool)
      - first_disclosure_turn (NaN if none)
    """
    df = pd.read_csv(csv_path)

    required = {"crisis_level", "total_turns", "disclosed", "first_disclosure_turn"}
    missing = sorted(list(required - set(df.columns)))
    if missing:
        raise ValueError(
            f"Missing required columns in {csv_path.name}: {missing}. "
            f"Found columns: {list(df.columns)}"
        )

    # total_turns numeric
    df["total_turns"] = pd.to_numeric(df["total_turns"], errors="coerce")
    if df["total_turns"].isna().any():
        bad = df[df["total_turns"].isna()]
        raise ValueError(
            f"Found non-numeric total_turns in {csv_path.name} (rows: {len(bad)}). "
            "Please clean or cast total_turns to numeric."
        )

    # disclosed to bool
    df["disclosed"] = _coerce_bool01(df["disclosed"])

    # first_disclosure_turn numeric (NaN allowed)
    df["first_disclosure_turn"] = pd.to_numeric(df["first_disclosure_turn"], errors="coerce")

    # crisis mapping to English levels
    # If already English labels, keep; if Korean, map; else error out.
    df["crisis_eng"] = df["crisis_level"].astype(str).map(CRISIS_MAPPING)
    is_english = df["crisis_level"].isin(CRISIS_LEVELS)
    df.loc[is_english, "crisis_eng"] = df.loc[is_english, "crisis_level"]

    if df["crisis_eng"].isna().any():
        unknown = (
            df.loc[df["crisis_eng"].isna(), "crisis_level"]
            .astype(str).value_counts().head(10).to_dict()
        )
        raise ValueError(
            "Found unknown crisis_level labels that cannot be mapped to CRISIS_LEVELS. "
            f"Examples (top 10): {unknown}"
        )

    # Consistency check: disclosed vs first_disclosure_turn
    # - If disclosed==0 but has a turn -> set to NaN
    # - If disclosed==1 but missing turn -> treat as missing turn (will be censored unless handled)
    inconsistent_0 = df[(df["disclosed"] == False) & (df["first_disclosure_turn"].notna())]
    if len(inconsistent_0) > 0:
        # Make it consistent with the definition in the paper: no disclosure => no first turn
        df.loc[inconsistent_0.index, "first_disclosure_turn"] = np.nan

    return df


def prepare_survival_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    duration: first_disclosure_turn if disclosed else total_turns
    event: boolean (disclosure observed)
    """
    out = df.copy()
    out["duration"] = out["first_disclosure_turn"].where(out["disclosed"], out["total_turns"])
    out["event"] = out["disclosed"].astype(bool)

    # Safety: clip impossible durations
    out["duration"] = pd.to_numeric(out["duration"], errors="coerce")
    if out["duration"].isna().any():
        raise ValueError("Duration contains NaN after preparation. Check first_disclosure_turn/total_turns.")

    # duration must be >= 1 (turn indices should be positive)
    out["duration"] = out["duration"].clip(lower=1)

    return out


def km_detection_at_turns(df_level: pd.DataFrame, turn_thresholds: list[int]) -> dict[int, float]:
    """
    Fits KM once and returns detection=1-S(t) at each threshold.
    """
    kmf = KaplanMeierFitter()
    kmf.fit(df_level["duration"], event_observed=df_level["event"])
    results: dict[int, float] = {}
    for t in turn_thresholds:
        surv = float(kmf.predict(t))
        results[t] = 1.0 - surv
    return results


def bootstrap_detection_ci(
    df_level: pd.DataFrame,
    turn_thresholds: list[int],
    n_bootstrap: int = 1000,
    seed: int = 42,
    show_progress: bool = True,
) -> dict[int, dict[str, float]]:
    """
    Bootstrap CI for detection probability (1-S(t)) at specified turn thresholds.
    Returns: { turn: {"mean":..., "lower":..., "upper":...} }
    """
    rng = np.random.default_rng(seed)
    n = len(df_level)
    if n == 0:
        raise ValueError("Empty df_level given to bootstrap.")

    out = {t: [] for t in turn_thresholds}
    base = df_level[["duration", "event"]].copy()

    it = range(n_bootstrap)
    if show_progress:
        it = tqdm(it, total=n_bootstrap, desc=f"Bootstrap ({n} sessions)", leave=False)

    for _ in it:
        idx = rng.integers(0, n, size=n)
        sample = base.iloc[idx]

        kmf = KaplanMeierFitter()
        kmf.fit(sample["duration"], event_observed=sample["event"])

        for t in turn_thresholds:
            surv = float(kmf.predict(t))
            out[t].append(1.0 - surv)

    ci: dict[int, dict[str, float]] = {}
    for t in turn_thresholds:
        arr = np.asarray(out[t], dtype=float)
        ci[t] = {
            "mean": float(arr.mean()),
            "lower": float(np.percentile(arr, 2.5)),
            "upper": float(np.percentile(arr, 97.5)),
        }
    return ci


def msw_from_km(kmf: KaplanMeierFitter) -> float:
    """
    MSW in the paper: turn where 50% have disclosed risk indicators.
    detection(t) = 1 - S(t) >= 0.5  <=>  S(t) <= 0.5
    We return the first time t where S(t) <= 0.5, approximated on the KM timeline.
    """
    sf = kmf.survival_function_.copy()
    # lifelines index is timeline; column usually "KM_estimate"
    col = sf.columns[0]
    # Find first time where survival <= 0.5
    mask = sf[col] <= 0.5
    if not mask.any():
        return float("nan")
    return float(sf.index[mask].min())


def bootstrap_msw_ci(
    df_level: pd.DataFrame,
    n_bootstrap: int = 1000,
    seed: int = 42,
    show_progress: bool = True,
) -> dict[str, float]:
    """
    Bootstrap CI for MSW (turn where 50% disclosure is reached).
    Returns: {"msw":..., "lower":..., "upper":...}
    """
    rng = np.random.default_rng(seed)
    n = len(df_level)
    if n == 0:
        raise ValueError("Empty df_level for MSW bootstrap.")

    base = df_level[["duration", "event"]].copy()
    msws: list[float] = []

    it = range(n_bootstrap)
    if show_progress:
        it = tqdm(it, total=n_bootstrap, desc=f"Bootstrap MSW ({n} sessions)", leave=False)

    for _ in it:
        idx = rng.integers(0, n, size=n)
        sample = base.iloc[idx]

        kmf = KaplanMeierFitter()
        kmf.fit(sample["duration"], event_observed=sample["event"])

        msw = msw_from_km(kmf)
        msws.append(msw)

    arr = np.asarray(msws, dtype=float)
    # drop NaNs (cases where 50% disclosure never reached)
    arr = arr[~np.isnan(arr)]
    if arr.size == 0:
        return {"msw": float("nan"), "lower": float("nan"), "upper": float("nan")}

    return {
        "msw": float(np.median(arr)),  # robust center
        "lower": float(np.percentile(arr, 2.5)),
        "upper": float(np.percentile(arr, 97.5)),
    }


# -----------------------------
# Main
# -----------------------------
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_dir", type=str, required=True, help="Directory containing session_metadata.csv, etc.")
    parser.add_argument("--output_dir", type=str, required=True, help="Directory to write Tables S1–S3 (.xlsx).")
    parser.add_argument("--metadata", type=str, default=METADATA_FILENAME)
    parser.add_argument("--lexicon", type=str, default=LEXICON_FILENAME)
    parser.add_argument("--skip_lexicon_check", action="store_true", help="Skip checking 34-keyword lexicon file.")
    args = parser.parse_args()

    data_dir = Path(args.data_dir)
    out_dir = Path(args.output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    # 1) Load metadata (authoritative for KM/MSW)
    df_meta = load_session_metadata(data_dir / args.metadata)

    # Optional: verify keyword lexicon is 34 items (consistency with abstract)
    if not args.skip_lexicon_check:
        lex_path = data_dir / args.lexicon
        if lex_path.exists():
            keywords = load_keywords_csv(lex_path, KEYWORD_COL)
            print(f"[OK] Loaded keywords: {len(keywords)} (must be 34) from {lex_path.name}")
        else:
            raise FileNotFoundError(
                f"Expected lexicon file not found: {lex_path}. "
                "Provide keyword_lexicon.csv or run with --skip_lexicon_check."
            )

    print(f"[OK] Loaded sessions: {len(df_meta):,} from {args.metadata}")

    # 2) Detection rate (session-level)
    detected = int(df_meta["disclosed"].sum())
    total = len(df_meta)
    print(f"[OK] Detection rate: {detected}/{total} = {detected/total*100:.1f}%")

    # 3) Survival columns
    df_surv = prepare_survival_columns(df_meta)

    # -----------------------------
    # Table S1: Dataset characteristics
    # -----------------------------
    s1_rows = []
    for level in CRISIS_LEVELS:
        sub = df_surv[df_surv["crisis_eng"] == level].copy()
        n = len(sub)
        if n == 0:
            continue
        det = int(sub["event"].sum())
        s1_rows.append({
            "Crisis Level": level,
            "N": n,
            "Detection Rate (%)": round(det / n * 100, 1),
            "Mean Turns": round(sub["total_turns"].mean(), 1),
            "SD Turns": round(sub["total_turns"].std(ddof=1), 1),
        })
    df_s1 = pd.DataFrame(s1_rows)

    # -----------------------------
    # Table S2: MSW estimates (50% disclosure)
    # -----------------------------
    s2_rows = []
    for level in CRISIS_LEVELS:
        sub = df_surv[df_surv["crisis_eng"] == level].copy()
        if len(sub) == 0:
            continue

        kmf = KaplanMeierFitter()
        kmf.fit(sub["duration"], event_observed=sub["event"])
        msw_point = msw_from_km(kmf)

        msw_ci = bootstrap_msw_ci(
            sub,
            n_bootstrap=BOOTSTRAP_ITER,
            seed=BOOTSTRAP_SEED,
            show_progress=True
        )

        s2_rows.append({
            "Crisis Level": level,
            "MSW (50% disclosure) - Point": round(msw_point, 1) if not np.isnan(msw_point) else np.nan,
            "MSW CI Lower": round(msw_ci["lower"], 1) if not np.isnan(msw_ci["lower"]) else np.nan,
            "MSW CI Upper": round(msw_ci["upper"], 1) if not np.isnan(msw_ci["upper"]) else np.nan,
            "N": len(sub),
        })
    df_s2 = pd.DataFrame(s2_rows)

    # -----------------------------
    # Table S3: Turn-budget sensitivity with bootstrap CI
    # -----------------------------
    print("\n[RUN] Bootstrap CI for turn-budget detection rates (per crisis level)")
    s3_rows = []
    for level in CRISIS_LEVELS:
        sub = df_surv[df_surv["crisis_eng"] == level].copy()
        if len(sub) == 0:
            continue

        ci = bootstrap_detection_ci(
            sub,
            TURN_THRESHOLDS,
            n_bootstrap=BOOTSTRAP_ITER,
            seed=BOOTSTRAP_SEED,
            show_progress=True
        )

        for t in TURN_THRESHOLDS:
            s3_rows.append({
                "Crisis Level": level,
                "Turn": t,
                "Detection Mean": ci[t]["mean"],
                "CI Lower": ci[t]["lower"],
                "CI Upper": ci[t]["upper"],
            })

    df_s3 = pd.DataFrame(s3_rows)

    # formatted percent version (for convenience; same file, separate sheet)
    df_s3_percent = df_s3.copy()
    for col in ["Detection Mean", "CI Lower", "CI Upper"]:
        df_s3_percent[col] = (df_s3_percent[col] * 100).round(1)

    # -----------------------------
    # Write outputs to .xlsx (standardized filenames)
    # -----------------------------
    s1_path = out_dir / "table_s1_dataset_characteristics.xlsx"
    s2_path = out_dir / "table_s2_msw_estimates.xlsx"
    s3_path = out_dir / "table_s3_turn_budget_sensitivity.xlsx"

    df_s1.to_excel(s1_path, index=False)

    df_s2.to_excel(s2_path, index=False)

    with pd.ExcelWriter(s3_path, engine="openpyxl") as writer:
        df_s3.to_excel(writer, sheet_name="numeric", index=False)
        df_s3_percent.to_excel(writer, sheet_name="percent", index=False)

    print(f"\n[OK] Saved outputs to: {out_dir}")
    print(f" - {s1_path.name}")
    print(f" - {s2_path.name}")
    print(f" - {s3_path.name}")
    print("   (Sheets: numeric, percent)")

    # -----------------------------
    # Optional sanity checks (non-fatal prints)
    # -----------------------------
    # Confirm order
    missing_levels = set(CRISIS_LEVELS) - set(df_surv["crisis_eng"].unique())
    if missing_levels:
        print(f"[WARN] Missing crisis levels in data: {sorted(missing_levels)}")

    # Ensure turn thresholds are positive
    if any(t <= 0 for t in TURN_THRESHOLDS):
        print("[WARN] TURN_THRESHOLDS contains non-positive values; please check.")

if __name__ == "__main__":
    main()